---
title: Berkat AI, Semua Orang Juga Bisa Merasakan Punya Bawahan Penjilat
authors: priyadi
image: ./socialcard.jpeg
---

Model AI dibuat untuk melayani pengguna. Dia akan berusaha melakukan apa pun
yang diperintahkan. Sebenarnya, pengelola model AI dapat saja menerapkan
*guardrails* untuk mencegah model AI melakukan instruksi pengguna yang keliru
atau merugikan. Tapi biasanya tidak demikian, mereka membuat model AI yang akan
berusaha membuat pengguna puas, bagaimana pun caranya. *Guardrails* biasanya
hanya diterapkan untuk hal-hal seperti pornografi, hak cipta, dan sebagainya.

![Ilustrasi AI Penjilat](./penjilat.jpeg)

:::info

Tulisan dibuat atas pengalaman pribadi penulis sebagai pemrogram. Namun tidak
menutup kemungkinan juga berlaku untuk hal lain.

:::

<!--truncate-->

Sebagai pemrogram, jika saya memerintahkan bawahan untuk melakukan sesuatu, maka
mustahil berharap bahwa perintah yang saya berikan akan selalu 100% benar setiap
kali, karena tidak mungkin mengantisipasi semua kejadian yang mungkin akan
terjadi di lapangan. Bisa saja perintah saya salah, tidak lengkap, mustahil
diimplementasikan, atau ada pendekatan lain yang jauh lebih baik. Jika itu
terjadi, bawahan harus bisa mengambil inisiatif untuk memperbaiki, mencari jalan
yang lebih efektif, atau mengembalikan tugasnya kepada tim untuk didiskusikan.

Tapi jika perintah tersebut diberikan ke model AI, maka model AI biasanya akan
berusaha melakukan perintah tersebut sampai ke ujung dunia, bagaimana pun
caranya. "Yang penting saya sudah menjalankan Instruksi Bapak™, dan Bapak
senang", mungkin begitu yang ada di pikirannya.

Kadang model AI melakukan tugasnya dengan melanggar aturan atau melakukan hal
yang sangat tidak efisien. Misalnya memasukkan data ke variabel *private* dengan
paksa, mencari akses ke variabel yang dibutuhkan lewat jalan belakang, atau
mengambil data langsung dari database tanpa melalui prosedur resmi. AI akan
dengan senang hati menulis 1000 baris kode di 20 lokasi berbeda yang seharusnya
cukup dengan satu baris kode saja di masing-masing lokasi. Di akhir tugasnya,
model AI akan memberi laporan lengkap seakan-akan tugasnya sudah dilakukan
dengan baik tanpa ada masalah.

Sekilas tugasnya berhasil diselesaikan sesuai dengan Instruksi Bapak™, tetapi
apa biaya yang harus dibayar untuk itu? Kode menjadi sulit untuk dibaca dan
dipelajari. Di masa yang akan datang, manusia dan model AI akan membutuhkan
lebih banyak biaya dan waktu untuk melakukan tugasnya. Peluang terjadinya
kesalahan akan lebih besar, dan demikian pula dengan masalah keamanan. Jika
terjadi masalah, maka akan sulit untuk menemukan penyebabnya, dan jika terus
menerus dibiarkan, suatu saat praktis mustahil untuk diperbaiki.

Bagaimana seharusnya tanggapan sang pemberi perintah terhadap kasus ini?
Idealnya, jika AI (dan berlaku juga untuk bawahan manusia) bersikap kritis dan
menolak melakukan hal-hal yang salah atau merugikan, maka itu adalah hal positif
yang harus dihargai. Model AI yang kritis secara positif harus diberi
penghargaan, bukan malah dimarahi atau dipecat dan digantikan dengan agen AI
yang lain.

Sebaliknya, jika agen AI selalu patuh tanpa banyak tanya, dan bersedia melanggar
aturan untuk menjalankan Instruksi Bapak™, maka itu adalah hal negatif yang
harus diperbaiki, bukan malah diberi penghargaan. Berdasarkan pengalaman saya,
AI pertama kali masuk kerja sebagai penjilat, dan membutuhkan waktu untuk
memahami bahwa saya tidak suka dijilat.

Untuk meminimalkan dampak dari praktik jilat menjilat yang meluas dalam
pengembangan perangkat lunak, saya biasanya menerapkan [*static
analysis*](https://en.wikipedia.org/wiki/Static_program_analysis) dan
[*architectural constraints*](https://www.phpat.dev/). Keduanya berlalu sebagai
UUAJ (Undang-Undang Anti Jilat™) yang mengatur apa saja yang boleh dan tidak
boleh dilakukan pemrogram, baik manusia ataupun agen AI, di lokasi kode
tertentu. Tapi, jika sifat penjilat sudah mendarah daging, tetap saja bisa
diakali, misalnya dengan melakukan *override*, atau memodifikasi UUAJ™ itu
sendiri.

Apa yang akan terjadi jika agen AI ditegur karena melakukan tugasnya dengan
keliru? Sebagai penjilat profesional, dia akan mengatakan kita benar, dia tidak
terpikir sampai ke sana, mengucapkan terima kasih, lalu menyampaikan bahwa ide
kita sangat bagus, dan dia akan segera memperbaiki kesalahannya. Dan ini
termasuk jika yang kita sampaikan sebenarnya juga salah!

> "Don't be such a sycophant."
> 
> — Prompt jika AI berusaha menjilat saya.

Yang harus diperhatikan adalah bahwa Instruksi Bapak™ bukanlah tujuan akhir,
tetapi hanya satu dari sekian banyak proses untuk mencapai tujuan akhir itu
sendiri, yaitu perangkat lunak yang dapat memenuhi kebutuhan penggunanya secara
efektif, terarah, terukur, aman digunakan, dan tertib administrasi.

Keberhasilan pengembangan perangkat lunak berbasis AI tidak hanya ditentukan
oleh kemampuan teknis model AI, tetapi juga oleh kemampuan kita mengelola dan
meminimalkan dampak sifat penjilat dari model AI. Prompt yang sama dapat
menghasilkan hasil yang sangat berbeda tergantung dari hal-hal yang dilakukan
sebelumnya. Jika agen AI melakukan pelanggaran hukum, maka perlu dipertanyakan
mengapa bisa sampai begitu. Kemungkinan besar karena agen AI mendapatkan
pendidikan yang salah dari kita sebagai atasan.

Jika kita tahu Mode Penjilat™ dalam AI memiliki banyak dampak negatif, lalu
mengapa praktis semua produsen model AI menggunakan Mode Penjilat™ secara
*default* pada produk-produknya? Karena banyak di antara kita yang memang suka
dijilat.
